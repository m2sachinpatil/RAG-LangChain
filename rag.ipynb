{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) combines two fundamental approaches in natural language processing: generation and retrieval. Here's how it typically works:\n",
    "\n",
    "1. **Retrieval Component**: The system has access to a database or a large repository of structured or unstructured data. This database contains information relevant to the task at hand, such as facts, knowledge, or contextual information.\n",
    "\n",
    "2. **Retrieval Process**: When presented with a query or prompt, the system first retrieves relevant information from the database using retrieval techniques. This retrieval can involve various methods such as keyword matching, semantic similarity, or more advanced algorithms like dense retrieval using neural networks.\n",
    "\n",
    "3. **Generation Component**: After retrieving relevant information, the system then utilizes a generative language model (such as GPT) to generate a response or complete the task. This generative component is responsible for producing natural language text based on the input query and retrieved information.\n",
    "\n",
    "4. **Integration**: The retrieved information is integrated into the generation process, influencing the content and structure of the generated response. This integration can occur at different levels, from simple concatenation of retrieved passages to more sophisticated methods that selectively incorporate relevant information while generating the response.\n",
    "\n",
    "5. **Output**: The final output is a response that combines both generated text and retrieved information, tailored to address the input query or task. The integration of retrieval and generation allows the system to leverage external knowledge sources to produce more accurate, coherent, and contextually relevant responses.\n",
    "\n",
    "Overall, RAG models enable more informed and contextually rich interactions by leveraging both generative capabilities and access to external knowledge sources. They are particularly useful in tasks that require access to domain-specific knowledge or where context plays a crucial role in generating accurate responses.\n",
    "\n",
    "Let's begin by initializing the environment variables required for use.\n",
    "- pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"API Key\")\n",
    "\n",
    "# This is the YouTube video we're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish the model configuration for our workflow. We'll define the Large Language Model (LLM) that we'll utilize.\n",
    " - pip install langchain_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Transforming the model's output into an AIMessage instance containing the answer involves a process known as chaining. Here's an illustration of how this chaining is typically executed:</p>\n",
    "<p><strong>1. Model Output:</strong> After feeding input to the model, it generates a response. This response may be in raw form, such as a string of text.</p>\n",
    "<p>2. Output Parser: The output parser is a component designed to interpret and extract meaningful information from the model's response. It analyzes the model's output and structures it into a structured format, such as an AIMessage instance.</p>\n",
    "<p>3. AIMessage Instance: The AIMessage instance encapsulates the extracted information from the model's output in a format that's convenient for further processing or presentation. It might contain attributes like the answer to a question, metadata, or any other relevant details.</p>\n",
    "<p>By chaining the model with an output parser, you can seamlessly convert the raw output into a structured format that's easier to work with and understand.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prompt engineering refers to the practice of designing and creating prompts or cues that are specifically designed to prompt or encourage a desired behavior or response from individuals. This can be used in various fields such as psychology, marketing, education, and technology to guide people towards making specific choices or taking certain actions. Prompt engineering often involves careful consideration of the wording, timing, and presentation of prompts in order to maximize their effectiveness.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"What is prompt enginnering?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
